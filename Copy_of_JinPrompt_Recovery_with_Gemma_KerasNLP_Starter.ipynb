{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 67121,
          "databundleVersionId": 7806901,
          "sourceType": "competition"
        },
        {
          "sourceId": 7731345,
          "sourceType": "datasetVersion",
          "datasetId": 4517764
        },
        {
          "sourceId": 7733314,
          "sourceType": "datasetVersion",
          "datasetId": 4518936
        },
        {
          "sourceId": 10261,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5388
        },
        {
          "sourceId": 11372,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5388
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinthakur/7ParkPublic/blob/master/Copy_of_JinPrompt_Recovery_with_Gemma_KerasNLP_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llm-prompt-recovery:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F67121%2F7806901%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T114731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D911729bd928db1077345c42bdd8a0dde0513b264e27bb565f43862ba9104f4cee159a15095d8f90a9a20fa6e608b8a4ea678da622d773aec50a3d063d50644f0d86f69174765977feefd3662e7bf430fef4b861ba256ac22eb11c49a479c657bb2b654325dff8d8a921730151e743bfb0aae6e362fd5ae522a8b38e4584d187ab922898816cc31ce93b19cdf407852b0dcc0bf8f6d8574a0d150810e87b34e41496d5579edd2a5552b38dfc75d13dd3731d78ac70d12e92ead108d43cf9c91b860658136762513c499ba4480de8f174257d6c6b0a3107f366e76f0030480db2d2775f7786a42131f2154d426bd5477c0e1ae59fd5337b7dca112d043c150ba91,llm-prompt-recovery-synthetic-datastore:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4517764%2F7731345%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T114731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5832b340538c8cf54ed7f2399a62281e3d7928b935ee8b59ed45c0d48f28fdb497f9b701db1804b6d376179f5726a05f4d0e395acb92e23f77aeeb0c6bbf3d81563476cdeff6f6c10515b6a90befa9407db9c6e9c5a6fa1f178cd128236679825761e14dc85bf546a7acaeded28bc7e7082c487dadb7bea8814c1752ea475b30555b05af0fb3615c15c2f087d224a78c2e18c1ff8029f6a0bbf002f2abd064d8d16c59aca5a580b867b68e34f263088f930770b30c358478a5db87995ad6359b21a2c4fefb8b8895dd0643fa57a57fe140bcc6548c67d0145bd0d7859207e334584426491b489896455346f9352d478bd3cfafa9c8bc26e1e7164200f614c55b,3000-rewritten-texts-prompt-recovery-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4518936%2F7733314%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T114731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6c54e07e2d6691de2c8ef99cfd4ae1bb5f3f21dbb17c1c205b44dcb267a0b1ca256d22a644726ab6661913c5d761d56afe327c914b047d3e63c66404d08ac3edf1ff38affc6dd68b2f09165f72aa2c0b44303f5f81a706c1b289dea2eb5cee0b3efc5d23844520c4f879516520ce05c61214b2e88725b4e68ddac4ff31523e2e5b1cf0e4b4639ecefe3a8d131843eb6895e98dd7b3e32c5fb2c0ffd1730378555d7f10d1de638840f18b880f0d6a890cbe183dd1fa70b7e619de070a1d15556ca70148a07f2623e24d18e0b60245847f6ec508a2f7f6a5089278506b3d5fb90877378da0d6b202fed59de6b4c9d8a6dc94d70fa1b2969be4467c35402b54b763,gemma/keras/gemma_instruct_2b_en/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F5388%2F10261%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T114731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3De4d3d6a689ce310b50a853546626e3629f0eaa8ca907ef404d2d3b49da9b2cc4c056eb9139e64ef4d024fbe2cec6df451934c335391304fe94badc7f862ffb4ff28fa6f9e79ab4b63c3708ecc628d6f1d32fbb6574f3a4d131383126081662bda99e60bb49a1b0da17fda0ee3569d9b252c1d986480484dcfdd6fbe0b17870cda446fbe4f5b4476a68617fd99e0a0441a56c702705d75490c5cde37cc2c2791d3f8cf17a372958122cfa1aa33486d7c41c0ed4ccfd632478ab34f85dd5f89ec1142aeff354b6fb0b990cbe88f662239923b7f8b2b6f5ee7d998339f25d3dc72a75a8528c196c28365f2ca35f8756f3469f6057e628d90dec0fa5488a8e004da8,gemma/keras/gemma_instruct_2b_en/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F5388%2F11372%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240415%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240415T114731Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7fe074be01ea029eed1f9ab6bcf256b4df21ee9cb183ce616decffb1c790769870f0091185752f6a62a1cd81ea93da2ec20c9a6ed981a046fb99358ca88c1964606a65ca700c06a22f91fc3cf167f9b15411892f98d7f56273fc2bfc8720dc54bb14a40c2cba90122344bb62f63f61eec6b97e0f3ad34ccdd24f6fd34c4db2578b3ce90fed8afcc6fa86d386b0ad7ec21faa73e582b2cf0920dedc5749c5709944f6e4c12cf8e54db7424923e1eeebb096e856876eb9d118a9628ff879c32bc5e4c81acd6da8f98dfe2a7be99f713d013452776485474311e9fdd5bb0fd9e24f01e28ec0ffb59a41ce3d4b67dcad6fada9e1f5147fdc7b25f6695effcb3faae1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ji335kaLllaI"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
        "This starter notebook is provided by the Keras team.</center>"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "hKtHYnEmllaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Prompt Recovery with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://i.ibb.co/8xZNc32/Gemma.png\">\n",
        "</div>\n",
        "\n",
        "In this competition, the goal is to find the prompt used to transform a given text. Specifically, we're seeking the prompt or instruction used in the [Gemma 7B-it](https://www.kaggle.com/models/google/gemma/frameworks/pyTorch/variations/7b-it-quant) model to convert one text to another. Typically, large language models are instructed to transform one text to another style, but here we're tasked with the inverse: finding the instruction/prompt used for the transformation. This notebook walks you through fine-tuning the **Gemma 2b-it** model with LoRA for this prompt recovery task using KerasNLP. Witih KerasNLP, we can fine-tune with LoRA using just a few lines of code.\n",
        "\n",
        "**Fun fact**: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).\n",
        "\n",
        "**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/)."
      ],
      "metadata": {
        "id": "XTXrLUMtllaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "lpswOz_BllaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
        "#os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "#‘XLA_PYTHON_CLIENT_MEM_FRACTION’: ‘4.0’\n",
        "#!python -m pip install tensorflow[and-cuda]\n",
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas() # progress bar for pandas\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:10.391413Z",
          "iopub.execute_input": "2024-04-15T11:10:10.391805Z",
          "iopub.status.idle": "2024-04-15T11:10:15.24975Z",
          "shell.execute_reply.started": "2024-04-15T11:10:10.391772Z",
          "shell.execute_reply": "2024-04-15T11:10:15.248562Z"
        },
        "trusted": true,
        "id": "unKaGHAZllaM",
        "outputId": "8bd3d22f-901b-49b8-e290-f9c1dffe42d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-15 11:10:12.064600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-15 11:10:12.064661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-15 11:10:12.066152: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "oRiPXwGCllaV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlJeiQf0llaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    seed = 42\n",
        "    dataset_path = \"/kaggle/input/llm-prompt-recovery\"\n",
        "    preset = \"gemma_instruct_2b_en\" # name of pretrained Gemma\n",
        "    sequence_length = 512 # max size of input sequence for training\n",
        "    batch_size = 1 # size of the input batch in training\n",
        "    epochs = 1 # number of epochs to train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.251606Z",
          "iopub.execute_input": "2024-04-15T11:10:15.25241Z",
          "iopub.status.idle": "2024-04-15T11:10:15.257075Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.252381Z",
          "shell.execute_reply": "2024-04-15T11:10:15.256228Z"
        },
        "trusted": true,
        "id": "Rskx7VtfllaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducibility\n",
        "Sets value for random seed to produce similar result in each run."
      ],
      "metadata": {
        "id": "b0T6dWubllaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(CFG.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.258003Z",
          "iopub.execute_input": "2024-04-15T11:10:15.258336Z",
          "iopub.status.idle": "2024-04-15T11:10:15.275441Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.258302Z",
          "shell.execute_reply": "2024-04-15T11:10:15.274683Z"
        },
        "trusted": true,
        "id": "2KM4TEsmllaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "No training data is provided in this competition; in other words, we can use any openly available datasets for this competition. In this notebook, we will use two external datasets that utilize the **Gemma 7B** model to transform texts using prompts.\n",
        "\n",
        "**Data Format:**\n",
        "\n",
        "These datasets includes:\n",
        "- `original_text`: Input text/essay that needs to be transformed.\n",
        "- `rewrite_prompt`: Prompt/Instruction that was used in the Gemma LM to transform `original_text`. This is also our **target** for this competition.\n",
        "- `rewritten_text`: Output text that was generated by the Gemma model."
      ],
      "metadata": {
        "id": "R9DbvmF6llaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `LLM Prompt Recovery - Synthetic Datastore dataset` by @dschettler8845\n",
        "df1 = pd.read_csv(\"/kaggle/input/llm-prompt-recovery-synthetic-datastore/gemma1000_w7b.csv\")\n",
        "df1 = df1[[\"original_text\", \"rewrite_prompt\", \"gemma_7b_rewritten_text_temp0\"]]\n",
        "df1 = df1.rename(columns={\"gemma_7b_rewritten_text_temp0\":\"rewritten_text\"})\n",
        "df1.head(2)\n",
        "\n",
        "# `3000 Rewritten texts - Prompt recovery Challenge` by @dipamc77\n",
        "df2 = pd.read_csv(\"/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\")\n",
        "df2.head(2)\n",
        "\n",
        "# Merge all datasets\n",
        "df = pd.concat([df1, df2], axis=0)\n",
        "df = df.sample(2000).reset_index(drop=True) # to reduce training time we are only using 2k samples\n",
        "df.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.277353Z",
          "iopub.execute_input": "2024-04-15T11:10:15.277618Z",
          "iopub.status.idle": "2024-04-15T11:10:15.403369Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.277595Z",
          "shell.execute_reply": "2024-04-15T11:10:15.402339Z"
        },
        "trusted": true,
        "id": "5wn8P9KqllaX",
        "outputId": "20f5f941-cf8e-45e3-caff-9c314d9fcf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                       original_text  \\\n0  Story highlights New findings suggest sperm al...   \n1  Glasgow's Needy is a non-registered charity or...   \n2  Pune: In a bizarre case, a parrot accused of \"...   \n3  Berrya is a genus of evergreen trees with fibr...   \n4  The Libertarian Party of Oregon is a political...   \n\n                                      rewrite_prompt  \\\n0                Turn this into a programmer's code.   \n1   Make this a formal apology letter to a customer.   \n2          Convert this into a technique to be used.   \n3  Convert the text into the layout of a classic ...   \n4  Recast it as the backstory for a mysterious an...   \n\n                                      rewritten_text  \n0  ```python\\n# Code to summarize the text\\n\\nspe...  \n1  [Your Name]\\n[Your Address]\\nGlasgow, Scotland...  \n2  **Technique:**\\n\\n**Step 1: Gather evidence.**...  \n3  ## Level Layout: Berrya Forest\\n\\n**Background...  \n4  In the shadows of the Evergreen State of Orego...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_text</th>\n      <th>rewrite_prompt</th>\n      <th>rewritten_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Story highlights New findings suggest sperm al...</td>\n      <td>Turn this into a programmer's code.</td>\n      <td>```python\\n# Code to summarize the text\\n\\nspe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Glasgow's Needy is a non-registered charity or...</td>\n      <td>Make this a formal apology letter to a customer.</td>\n      <td>[Your Name]\\n[Your Address]\\nGlasgow, Scotland...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pune: In a bizarre case, a parrot accused of \"...</td>\n      <td>Convert this into a technique to be used.</td>\n      <td>**Technique:**\\n\\n**Step 1: Gather evidence.**...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Berrya is a genus of evergreen trees with fibr...</td>\n      <td>Convert the text into the layout of a classic ...</td>\n      <td>## Level Layout: Berrya Forest\\n\\n**Background...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Libertarian Party of Oregon is a political...</td>\n      <td>Recast it as the backstory for a mysterious an...</td>\n      <td>In the shadows of the Evergreen State of Orego...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "Here's a simple prompt template we'll use to create instruction-response pairs from the `original_text`, `rewritten_text`, and `rewritten_prompt`:\n",
        "\n",
        "```\n",
        "Instruction:\n",
        "Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the \"Original Text\" and \"Rewritten Text\", and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n",
        "\n",
        "Original Text:\n",
        "...\n",
        "\n",
        "Rewritten Text:\n",
        "...\n",
        "\n",
        "Response:\n",
        "...\n",
        "```\n",
        "\n",
        "This template will help the model to follow instruction and respond accurately. You can explore more advanced prompt templates for better results."
      ],
      "metadata": {
        "id": "SQDugK4vllaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Instruction:\\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\\n\\nOriginal Text:\\n{original_text}\\n\\nRewriten Text:\\n{rewritten_text}\\n\\nResponse:\\n{rewrite_prompt}\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.404929Z",
          "iopub.execute_input": "2024-04-15T11:10:15.405202Z",
          "iopub.status.idle": "2024-04-15T11:10:15.40976Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.405178Z",
          "shell.execute_reply": "2024-04-15T11:10:15.408786Z"
        },
        "trusted": true,
        "id": "b8naoFuWllaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"prompt\"] = df.progress_apply(lambda row: template.format(original_text=row.original_text,\n",
        "                                                             rewritten_text=row.rewritten_text,\n",
        "                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\n",
        "data = df.prompt.tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.410945Z",
          "iopub.execute_input": "2024-04-15T11:10:15.411293Z",
          "iopub.status.idle": "2024-04-15T11:10:15.525251Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.411261Z",
          "shell.execute_reply": "2024-04-15T11:10:15.524216Z"
        },
        "trusted": true,
        "id": "TGTlSaWJllaZ",
        "outputId": "741870df-f3ca-4546-b3bd-df2e3189f4e0",
        "colab": {
          "referenced_widgets": [
            "e9fc01b7551d47398f14b87308e153fc"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2000 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9fc01b7551d47398f14b87308e153fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting."
      ],
      "metadata": {
        "id": "mlDm1mLFllaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "<div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div>\n",
        "\n",
        "**Gemma** is a collection of advanced open LLMs developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n",
        "\n",
        "Gemma models are available in several sizes so we can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n",
        "\n",
        "| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n",
        "|-----------------|-------------------|------------------------------------|------------------------|\n",
        "| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n",
        "| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n",
        "| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n",
        "| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n",
        "\n",
        "In this notebook, we will utilize the `Gemma 2b-it` model from KerasNLP's pretrained models to recover the prompt. We are using the \"Instruction tuned\" model instead of the \"Pretrained\" one because the test data was generated from an instruction-tuned Gemma model. Additionally, we will fine-tune our model using instruction-response pairs thus fine-tuning an instruction-tuned model will likely yield better results.\n",
        "\n",
        "To explore other available models, you can simply adjust the `preset` value in the `CFG` (config). You can find a list of other pretrained models on the [KerasNLP website](https://keras.io/api/keras_nlp/models/)."
      ],
      "metadata": {
        "id": "WXKZksanllaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma Causal LM\n",
        "\n",
        "The code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n",
        "\n",
        "This model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n",
        "\n",
        "> The `from_preset` method instantiates the model from a preset architecture and weights."
      ],
      "metadata": {
        "id": "inXNT0SdllaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.preset)\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:10:15.526819Z",
          "iopub.execute_input": "2024-04-15T11:10:15.527181Z",
          "iopub.status.idle": "2024-04-15T11:11:38.868782Z",
          "shell.execute_reply.started": "2024-04-15T11:10:15.527143Z",
          "shell.execute_reply": "2024-04-15T11:11:38.867818Z"
        },
        "trusted": true,
        "id": "bFm7WTOrllaZ",
        "outputId": "f712f1ca-30bf-4fa0-bae8-9227b5e90b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma LM Preprocessor\n",
        "\n",
        "An important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n",
        "\n",
        "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
        "\n",
        "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
        "\n",
        "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
        "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
        "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
      ],
      "metadata": {
        "id": "DgFYF4fYllaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, sample_weight = gemma_lm.preprocessor(data[0:2])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:11:38.870041Z",
          "iopub.execute_input": "2024-04-15T11:11:38.870372Z",
          "iopub.status.idle": "2024-04-15T11:11:39.18823Z",
          "shell.execute_reply.started": "2024-04-15T11:11:38.870345Z",
          "shell.execute_reply": "2024-04-15T11:11:39.187168Z"
        },
        "trusted": true,
        "id": "Gh0FIvM3llaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n",
        "\n",
        "From the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`."
      ],
      "metadata": {
        "id": "RH8f26ijllaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of each processed output\n",
        "for k, v in x.items():\n",
        "    print(k, \":\", v.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:11:39.189986Z",
          "iopub.execute_input": "2024-04-15T11:11:39.190386Z",
          "iopub.status.idle": "2024-04-15T11:11:39.196251Z",
          "shell.execute_reply.started": "2024-04-15T11:11:39.19035Z",
          "shell.execute_reply": "2024-04-15T11:11:39.195311Z"
        },
        "trusted": true,
        "id": "Rm1XnblLllaa",
        "outputId": "a0ac92f9-4c3d-4f62-b717-b361af374d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "token_ids : (2, 8192)\npadding_mask : (2, 8192)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference before Fine-Tuning\n",
        "\n",
        "Before we do fine-tuning, let's try to recover the prompt using the Gemma model with some prepared prompts and see how it responds.\n",
        "\n",
        "> As this model is not yet fine-tuned for instruction, you will notice that the model's responses are inaccurate."
      ],
      "metadata": {
        "id": "kMl2s16Qllaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n",
        "                           [\"red\", \"yellow\", \"blue\", \"green\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "trusted": true,
        "id": "iM1RGCWHllaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 1"
      ],
      "metadata": {
        "id": "l64AoxJJllaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[10]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "    original_text=row.original_text,\n",
        "    rewritten_text=row.rewritten_text,\n",
        "    rewrite_prompt=\"\",\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "# Colorize\n",
        "#output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-15T11:11:39.200334Z",
          "iopub.execute_input": "2024-04-15T11:11:39.20062Z",
          "iopub.status.idle": "2024-04-15T11:12:12.402621Z",
          "shell.execute_reply.started": "2024-04-15T11:11:39.200597Z",
          "shell.execute_reply": "2024-04-15T11:12:12.401641Z"
        },
        "trusted": true,
        "id": "9AghmZRRllab",
        "outputId": "d5625d3f-dccf-474a-d33e-e98621045e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Instruction:\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\nOriginal Text:\nZoltán Tóth (born 24 August 1979) is a Hungarian former competitive figure skater. He is a five-time Hungarian national champion and competed in two Winter Olympics.\n\nRewriten Text:\nSure, here's the rewritten text as if it were a list of ingredients in a magical potion:\n\nThe ingredients to create the potion:\n\nZoltán Tóth's birth date: 24 August 1979\nHungarian heritage\nFive-time Hungarian national champion\nTwo Winter Olympics participation\n\nResponse:\nThe LLM was likely given the prompt or instruction to create a list of ingredients for a magical potion, based on the information in the original text."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 2"
      ],
      "metadata": {
        "id": "o2bBZyaSllab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[20]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "    original_text=row.original_text,\n",
        "    rewritten_text=row.rewritten_text,\n",
        "    rewrite_prompt=\"\",\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "# Colorize\n",
        "#output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-04-15T11:12:12.403729Z",
          "iopub.execute_input": "2024-04-15T11:12:12.404021Z",
          "iopub.status.idle": "2024-04-15T11:12:21.093189Z",
          "shell.execute_reply.started": "2024-04-15T11:12:12.403996Z",
          "shell.execute_reply": "2024-04-15T11:12:21.092112Z"
        },
        "trusted": true,
        "id": "eTV0FLjQllab",
        "outputId": "fa6c8ee8-954d-4b97-f4a4-517fb5dcb944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Instruction:\nBelow, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\nOriginal Text:\nAbu al-Hassan Ali ibn Moussa ibn Rashid al-Alami (), also known as Sherif Moulay Ali Ben Rachid, was the founder of the city of Chefchaouen, Morocco. He was an Idrisid and descendant of Sufi saint Abd as-Salam ibn Mashish al-Alami. He was also the father of Sayyida al-Hurra, governor of Tetouan.\n\nRewriten Text:\n## Internal Audit Checklist - Governance and Historical Background\n\n**Objective:** To assess the accuracy and completeness of the historical narrative regarding the founding of Chefchaouen and the related individuals' identities and achievements.\n\n**Criteria:**\n\n**1. Accuracy of the historical narrative:**\n\n- Does the text accurately describe Abu al-Hassan Ali ibn Moussa ibn Rashid al-Alami's role as the founder of Chefchaouen?\n- Does the text accurately describe the Idrisid heritage and Sufi lineage of the founder?\n- Is the information about the founder's relationship to Sayyida al-Hur\n\nResponse:\n**Yes, the rewritten text is largely accurate and provides a good overview of the historical narrative.**\n\n**2. Completeness of the historical narrative:**\n\n- Does the text provide all the relevant historical information about the founding of Chefchaouen?\n- Are there any missing or omitted key details?\n- Is the text well-organized and easy to follow?\n\nResponse:\n**The rewritten text provides a good overview of the historical narrative, but there are a few minor omissions and missing details that could be further addressed for greater accuracy.**\n\n**3. Clarity and conciseness of the rewritten text:**\n\n- Is the text written in a clear and concise style?\n- Is the information presented in a logical and easy-to-understand manner?\n- Does the text use appropriate vocabulary and tone?\n\nResponse:\n**The rewritten text is well-written and clear, providing a concise and accurate overview of the historical narrative.**\n\n**4. Alignment with the original text"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with LoRA\n",
        "\n",
        "To get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA).\n",
        "\n",
        "**What exactly is LoRA?**\n",
        "\n",
        "LoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n",
        "\n",
        "Imagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\n",
        "Credit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n",
        "\n",
        "\n",
        "In the LoRA paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n",
        "\n",
        "**Why does LoRA save memory?**\n",
        "\n",
        "Even though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage.\n",
        "\n",
        "> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters."
      ],
      "metadata": {
        "id": "Rhc35e-3llab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:12:21.094758Z",
          "iopub.execute_input": "2024-04-15T11:12:21.095078Z",
          "iopub.status.idle": "2024-04-15T11:12:21.579644Z",
          "shell.execute_reply.started": "2024-04-15T11:12:21.095052Z",
          "shell.execute_reply": "2024-04-15T11:12:21.578805Z"
        },
        "trusted": true,
        "id": "DJTiJjIMllac",
        "outputId": "7c66540c-a9a8-4610-ce3d-75cd8b24b829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA."
      ],
      "metadata": {
        "id": "7yvQMuPNllac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "wYsx993Wllac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 8192 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = CFG.sequence_length\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "#customoptimizer = keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))\n",
        "\n",
        "# Compile the model with loss, optimizer, and metric\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    #\n",
        "\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "gemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T11:12:21.580894Z",
          "iopub.execute_input": "2024-04-15T11:12:21.581556Z"
        },
        "trusted": true,
        "id": "dzyxEfdYllac",
        "outputId": "89661991-e707-43e1-84d4-00a54863854a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1463/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m12:36\u001b[0m 1s/step - loss: 1.3966 - sparse_categorical_accuracy: 0.6028",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference after fine-tuning\n",
        "\n",
        "Let's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model."
      ],
      "metadata": {
        "id": "MVPyQ3Rhllac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Instruction\", \"Original Text\", \"Rewriten Text\", \"Response\"],\n",
        "                           [\"red\", \"yellow\", \"blue\", \"green\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "qFkbjrpEllad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 1"
      ],
      "metadata": {
        "id": "ohRyI7Willad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[10]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "    original_text=row.original_text,\n",
        "    rewritten_text=row.rewritten_text,\n",
        "    rewrite_prompt=\"\",\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=512)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "xQZuto9-llad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 2"
      ],
      "metadata": {
        "id": "w0Jd-G2sllad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[20]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "    original_text=row.original_text,\n",
        "    rewritten_text=row.rewritten_text,\n",
        "    rewrite_prompt=\"\",\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=8192)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "y-gVjqdSllah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Data"
      ],
      "metadata": {
        "id": "DYT_TfHnllah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
        "test_df['original_text'] = test_df['original_text'].fillna(\"\")\n",
        "test_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\n",
        "test_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "iKKMlUbCllah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sample\n",
        "\n",
        "Now, let's try out a sample from test data that model hasn't seen during training."
      ],
      "metadata": {
        "id": "0jER5oN2llah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = test_df.iloc[0]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "    original_text=row.original_text,\n",
        "    rewritten_text=row.rewritten_text,\n",
        "    rewrite_prompt=\"\",\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=8192)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "UkoEvUkHllai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "PyQNGZNOllai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    row = test_df.iloc[i]\n",
        "\n",
        "    # Generate Prompt using template\n",
        "    prompt = template.format(\n",
        "        original_text=row.original_text,\n",
        "        rewritten_text=row.rewritten_text,\n",
        "        rewrite_prompt=\"\"\n",
        "    )\n",
        "\n",
        "    # Infer\n",
        "    output = gemma_lm.generate(prompt, max_length=8192)\n",
        "    pred = output.replace(prompt, \"\") # remove the prompt from output\n",
        "\n",
        "    # Store predictions\n",
        "    preds.append([row.id, pred])"
      ],
      "metadata": {
        "trusted": true,
        "id": "OaK7qFjallai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While preparing the submission file, we must keep in mind that, leaving any `rewrite_prompt` blank as null answers will throw an error."
      ],
      "metadata": {
        "id": "RARTYo4Cllai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\n",
        "sub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\n",
        "sub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Improve the essay\" if len(x) == 0 else x)\n",
        "sub_df.to_csv(\"submission.csv\",index=False)\n",
        "sub_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SGpuzFNCllai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The result is pretty good. Still there is ample room for improvement. Here are some tips to improve performance:\n",
        "\n",
        "- Try using the larger version of **Gemma** (7B).\n",
        "- Increase `sequence_length`.\n",
        "- Experiment with advanced prompt engineering techniques.\n",
        "- Implement augmentation to increase the number of samples.\n",
        "- Utilize a learning rate scheduler."
      ],
      "metadata": {
        "id": "u0RcmpGKllai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n",
        "* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n",
        "* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)"
      ],
      "metadata": {
        "id": "1roMzfPZllaj"
      }
    }
  ]
}